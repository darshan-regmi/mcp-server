# MCP Server

MCP (Model Context Protocol) Server is a modular AI assistant engine that runs on your machine to extend what a language model can do beyond just generating text.

## Features

- ğŸ§  AI Router: Switch between online APIs (OpenRouter, OpenAI) and local models (via Ollama)
- ğŸ› ï¸ Tool Plugins: Poetry generation, code assistance, Mac cleanup, daily agent, and more
- ğŸ“š Knowledge & Memory: Vector database storage for personal data and conversations
- ğŸ™ï¸ Multimodal: Speech-to-text, text-to-speech, and image processing capabilities
- ğŸ“¶ API Clients: OpenRouter.ai, HuggingFace, Google Search, WolframAlpha

## Use Cases

- âœï¸ **Poetry Mode**: Generate, complete, or enhance verses based on your own poetry database
- ğŸ§‘â€ğŸ’» **Coding Mode**: Code generation, debugging, snippet search, and more
- ğŸ¯ **Life Assistant Mode**: Daily briefings, schedule tracking, and task management
- ğŸ“š **Education & Research**: Topic explanations, research article summarization
- ğŸ§  **Custom Persona**: Create a chatbot that reflects your unique style and preferences

## Installation

1. Clone this repository:
```bash
git clone https://github.com/darshan-regmi/mcp-server.git
cd mcp-server
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Create a `.env` file based on the example:
```bash
cp .env.example .env
# Edit .env with your API keys
```

4. Start the server:
```bash
python -m app.main
```

5. Open your browser and navigate to:
```
http://localhost:8000
```

## API Keys

MCP Server can use various AI models and services. You'll need to obtain API keys for the services you want to use:

- **OpenAI API**: https://platform.openai.com/
- **OpenRouter**: https://openrouter.ai/
- **HuggingFace**: https://huggingface.co/
- **Google Custom Search**: https://developers.google.com/custom-search/
- **Wolfram Alpha**: https://developer.wolframalpha.com/
- **ElevenLabs**: https://elevenlabs.io/

For offline usage, you can use [Ollama](https://ollama.ai/) to run models locally.

## Project Structure

```
mcp-server/
â”œâ”€â”€ app/                  # Main application code
â”‚   â”œâ”€â”€ routers/          # API routers
â”‚   â”œâ”€â”€ static/           # Web interface
â”‚   â””â”€â”€ main.py           # Application entry point
â”œâ”€â”€ tools/                # Tool plugins
â”œâ”€â”€ memory/               # Vector database storage
â”œâ”€â”€ config/               # Configuration files
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ .env                  # Environment variables (not in repo)
```

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### Additional Setup

1. Install Ollama for local model support (optional):
   ```bash
   # Visit https://ollama.ai/ for installation instructions
h   ```

## Usage

### Starting the Server

```bash
python -m app.main
```

You can configure the server using environment variables in your `.env` file:
- `MCP_HOST`: Host address to bind to (default: 0.0.0.0)
- `MCP_PORT`: Port to listen on (default: 8000)
- `MCP_MEMORY_DIR`: Directory for storing memory data (default: memory)

The server will be available at http://localhost:8000 by default.

### API Endpoints

#### Chat

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Write a poem about dreams", "mode": "poetry"}'
```

#### Tools

```bash
curl -X POST http://localhost:8000/tools/execute \
  -H "Content-Type: application/json" \
  -d '{"tool_name": "poetry_gen", "parameters": {"topic": "dreams", "style": "sonnet"}}'
```

#### Memory

```bash
curl -X POST http://localhost:8000/memory/search \
  -H "Content-Type: application/json" \
  -d '{"query": "What did I write about dreams?", "collection": "notes"}'
```

## Architecture

```
ğŸ“¦ MCP Server (FastAPI + Python)
â”œâ”€â”€ ğŸ§  AI Router (API vs Local LLM via Ollama)
â”œâ”€â”€ ğŸ› ï¸ Tool Plugins
â”‚   â”œâ”€â”€ poetry_gen.py
â”‚   â”œâ”€â”€ code_assist.py
â”‚   â”œâ”€â”€ mac_cleanup.py
â”‚   â”œâ”€â”€ daily_agent.py
â”‚   â””â”€â”€ memory_manager.py
â”œâ”€â”€ ğŸ“š Knowledge & Memory
â”‚   â”œâ”€â”€ Local Notes (MD, TXT, Notion export)
â”‚   â”œâ”€â”€ Vector Store (ChromaDB / Qdrant)
â”‚   â””â”€â”€ Browser Tools / Docs Reader
â”œâ”€â”€ ğŸ™ï¸ Multimodal
â”‚   â”œâ”€â”€ Speech-to-text (Whisper)
â”‚   â”œâ”€â”€ TTS (Piper / ElevenLabs)
â”‚   â””â”€â”€ Image Tools
â””â”€â”€ ğŸ“¶ API Clients
    â”œâ”€â”€ OpenRouter.ai
    â”œâ”€â”€ HuggingFace
    â”œâ”€â”€ Google Search
    â””â”€â”€ WolframAlpha
```

### API Response Format

```json
{
  "message": "Here's a sonnet about dreams...",
  "tool_calls": [
    {
      "name": "poetry_gen",
      "parameters": {"topic": "dreams", "style": "sonnet"}
    }
  ],
  "sources": [
    {"title": "My Dream Journal", "content": "Excerpt from your notes..."}
  ],
  "mode": "poetry",
  "model_used": "claude-3-opus-20240229",
  "processing_time": 1.25
}
```

## Privacy and Security

- ğŸ”’ Your data stays local - no one sees your personal information
- ğŸ”‘ API keys are stored securely in your `.env` file
- ğŸ›¡ï¸ Offline-capable with local models via Ollama
- ğŸŒ Consider using SSL/TLS for secure communication in production
- ğŸ”¥ Restrict access to the MCP server port using a firewall

## Extending MCP Server

You can extend the MCP server by adding your own tool plugins:

1. Create a new Python file in the `tools` directory
2. Define a class that inherits from the `Tool` base class
3. Implement the `execute` method
4. Your tool will be automatically loaded when the server starts

## License

This project is open source and available under the MIT License.
